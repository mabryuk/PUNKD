{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependencies and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gdown\n",
    "import patoolib\n",
    "\n",
    "from punkd.preprocessing import get_data, get_transforms\n",
    "from monai.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from punkd.unet import FlexUNet, train_epoch, validate_epoch\n",
    "from punkd.unet import student_train_epoch\n",
    "\n",
    "#from monai.losses import DiceLoss\n",
    "#from monai.metrics import DiceMetric\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from tqdm.notebook import tqdm\n",
    "from punkd.diceMetrices import DiceLoss, DiceScore\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# R2 credentials and endpoint\n",
    "access_key = os.environ.get('R2_KEY')\n",
    "secret_key = os.environ.get('R2_SECRET')\n",
    "endpoint_url = os.environ.get('R2_ENDPOINT')\n",
    "bucket_name = os.environ.get('R2_BUCKET')\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3',\n",
    "\t\t\t\t  endpoint_url=endpoint_url,\n",
    "\t\t\t\t  aws_access_key_id=access_key,\n",
    "\t\t\t\t  aws_secret_access_key=secret_key,\n",
    "\t\t\t\t  config=Config(signature_version='s3v4'))\n",
    "\n",
    "\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load configuration from config.json file in the root directory\n",
    "config = None\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('archive.zip') and not os.path.exists('data'):\n",
    "    # Google Drive file ID\n",
    "    file_id = '1bz476ATbSduGcyw1UIkOkN5YkYZvurZ5'\n",
    "    # Destination path where the file will be saved\n",
    "    destination = 'archive.zip'\n",
    "\n",
    "    # Construct the download URL\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "    # Download the file\n",
    "    gdown.download(url, destination, quiet=False)\n",
    "    \n",
    "    patoolib.extract_archive('archive.zip', outdir='data')\n",
    "    os.remove('archive.zip')\n",
    "\n",
    "if os.path.exists('archive.zip'):\n",
    "    patoolib.extract_archive('archive.zip', outdir='data')\n",
    "    os.remove('archive.zip')\n",
    "\n",
    "if os.path.exists('data/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii'):\n",
    "    # rename the file\n",
    "    os.rename('data/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/W39_1998.09.19_Segm.nii', \n",
    "                'data/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_355/BraTS20_Training_355_seg.nii')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files = get_data()\n",
    "transforms = get_transforms()\n",
    "\n",
    "print(f'Train files: {len(train_files)}')\n",
    "print(f'Test files: {len(val_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(data=train_files, transform=transforms)\n",
    "val_ds = Dataset(data=val_files, transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPOCHS = config['EPOCHS']\n",
    "BATCH_SIZE_TRAIN = config['TRAIN_BATCH_SIZE']\n",
    "BATCH_SIZE_VAL = config['VAL_BATCH_SIZE']\n",
    "\n",
    "LR = config['LEARNING_RATE']\n",
    "WORKERS = config['CPU_WORKERS']\n",
    "\n",
    "TEACHER_MODEL_SIZE = config['TEACHER_MODEL_SIZE']\n",
    "STUDENT_MODEL_SIZE = config['STUDENT_MODEL_SIZE']\n",
    "\n",
    "temperature = config[\"TEMPERATURE\"]\n",
    "alpha = config[\"ALPHA\"]\n",
    "\n",
    "obj_name = 'teacher_model'\n",
    "file_name = obj_name + '.pth'\n",
    "\n",
    "if not os.path.exists(file_name):\n",
    "    s3.download_file(bucket_name, file_name, file_name)\n",
    "\n",
    "\n",
    "model = FlexUNet(model_size=TEACHER_MODEL_SIZE).to(device)\n",
    "teacher_model = FlexUNet(model_size=TEACHER_MODEL_SIZE).to(device)\n",
    "teacher_model.load_state_dict(torch.load(file_name))  # Load pre-trained teacher model\n",
    "teacher_model.eval()\n",
    "\n",
    "student_model = FlexUNet(model_size=STUDENT_MODEL_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=LR)\n",
    "T_max = EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=1e-4)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE_TRAIN, shuffle=True, num_workers=WORKERS)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "loss_fn = F.kl_div  \n",
    "score_fn = DiceScore().to(device)\n",
    "\n",
    "student_model_id = 'UNET_32_Student_01'\n",
    "model_file = student_model_id + '.pth'\n",
    "csv_file = student_model_id + '.csv'\n",
    "\n",
    "performance = pd.DataFrame(columns=['epoch', 'train_loss', 'train_dice', 'tumor_dice', 'enhancing_dice', 'core_dice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_train_dice = float('-inf')\n",
    "\n",
    "with tqdm(total=EPOCHS, unit='epoch', position=0) as pbar:\n",
    "    pbar.set_description(f'EPOCH {0}')\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss = student_train_epoch(student_model, teacher_model, train_loader, optimizer, device, temperature,         # Knowledge distillation temperature\n",
    "           alpha  )\n",
    "        total, (background, tumor, enhancing, core) = validate_epoch(student_model, val_loader, score_fn, device)\n",
    "        if total > best_train_dice:\n",
    "            best_train_dice = total\n",
    "            torch.save(student_model.state_dict(), model_file)\n",
    "            print(f\"New best model saved at epoch {epoch+1} with train_dice: {best_train_dice:.4f}\")\n",
    "        pbar.set_description(f'EPOCH {epoch+1}')\n",
    "        pbar.set_postfix({'Dice Loss': loss, 'Total Score': total, 'Tumor Score': tumor, 'Enhancing Score': enhancing, 'Core Score': core})\n",
    "        perf = pd.DataFrame({'epoch': [epoch+1], 'train_loss': [loss], 'train_dice': [total], 'tumor_dice': [tumor],\n",
    "                                          'enhancing_dice': [enhancing], 'core_dice': [core]})\n",
    "        performance = pd.concat([performance, perf], ignore_index=True)\n",
    "       \n",
    "        pbar.update()\n",
    "        scheduler.step()\n",
    "\n",
    "performance.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model archiving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to upload file with progress bar\n",
    "def upload_file_with_progress(file_path, bucket_name, object_name):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    progress = tqdm(total=file_size, unit='B', unit_scale=True, desc='Uploading ' + object_name)\n",
    "\n",
    "    def upload_progress(chunk):\n",
    "        progress.update(chunk)\n",
    "\n",
    "    s3.upload_file(file_path, bucket_name, object_name, Callback=upload_progress)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "# Upload the file\n",
    "upload_file_with_progress(model_file, bucket_name, model_file)\n",
    "upload_file_with_progress(csv_file, bucket_name, csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
